<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement</title>
	<link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet"
            href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/index.js"></script>
    

</head>

<body>

    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement</h1>
            <!-- <div class="nerf_subheader_v2">Under Review</div> -->
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://scholar.google.com/citations?user=KMrFk2MAAAAJ&hl=zh-CN&oi=sra" target="_blank" class="nerf_authors_v2">Xu He<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://xiaoyu258.github.io/" target="_blank" class="nerf_authors_v2">Xiaoyu Li<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a  target="_blank" class="nerf_authors_v2">Di Kang<span
                        class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a  target="_blank" class="nerf_authors_v2">Jiangnan Ye<span
                        class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;                
                    <a  target="_blank" class="nerf_authors_v2">Chaopeng Zhang<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <br>
                    <a  target="_blank" class="nerf_authors_v2">Liyang Chen<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;                   
                    <a href="https://scholar.google.com/citations?user=qgdesEcAAAAJ&hl=en&oi=sra" target="_blank" class="nerf_authors_v2">Xiangjun Gao<span
                            class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;
                    <a target="_blank" class="nerf_authors_v2">Han Zhang<span
                            class="text-span_nerf"></span></a><sup> 4</sup>,&nbsp;&nbsp;
                    <a target="_blank" class="nerf_authors_v2">Zhiyong Wu<span
                            class="text-span_nerf"></span></a><sup> 1,5,&#9993;</sup>,&nbsp;&nbsp;
                    <a target="_blank" class="nerf_authors_v2">Haolin Zhuang<span
                        class="text-span_nerf"></span></a><sup> 1</sup>
                </div>
                <div>
                    <p class="nerf_affiliation_v2"><sup>1 </sup>Shenzhen International Graduate School, Tsinghua University</p>,&nbsp;&nbsp;
                    <p class="nerf_affiliation_v2"><sup>2 </sup>Tencent AI Lab</p>,
                    <br>
                    <p class="nerf_affiliation_v2"><sup>3 </sup>The Hong Kong University of Science and Technology</p>,&nbsp;&nbsp;
                    <p class="nerf_affiliation_v2"><sup>4 </sup>Stanford University</p>,&nbsp;&nbsp;
                    <p class="nerf_affiliation_v2"><sup>5 </sup>The Chinese University of Hong Kong</p>
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2408.14211" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="https://arxiv.org/pdf/2408.14211" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/thuhcsi/MagicMan" role="button" target="_blank">
                        <i class="fa-brands fa-github"></i> Code </a>
                    <!-- <a class="btn btn-large btn-light" href="https://youtu.be" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-youtube"></i> Video </a> -->
                </div>

            </div>
        </div>

    </div>
    


    <section class="hero is-light is-small" style="position: relative; z-index: 1;">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-video1">
                <video poster="" id="video1" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-1.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video2">
                <video poster="" id="video2" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-2.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video3">
                <video poster="" id="video3" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-3.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video4">
                <video poster="" id="video4" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-4.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video5">
                <video poster="" id="video5" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-5.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video6">
                <video poster="" id="video6" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-6.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="item item-video7">
                <video poster="" id="video7" controls playsinline height="100%" loop playsinline autoPlay muted controls>
                  <source src="assets/gallery-7.mp4"
                          type="video/mp4">
                </video>
              </div>

            </div>
          </div>
          <p class="myprompt comment_text" style="text-align: center;">
            Given a single reference image, <strong>MagicMan</strong> can generate dense, high-quality, and consistent human novel view images and normal maps. 
        </p>
        </div>
    </section>
      
      

    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf_abstract">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                Existing works in single-image human reconstruction suffer from weak generalizability 
                due to insufficient training data or 3D inconsistencies for a lack of comprehensive multi-view knowledge. 
                In this paper, we introduce <strong>MagicMan</strong>, a human-specific multi-view diffusion model designed to generate high-quality novel view images from a single reference image. 
                As its core, we leverage a pre-trained 2D diffusion model as the generative prior for generalizability, 
                with the parametric SMPL-X model as the 3D body prior to promote 3D awareness. 
                To tackle the critical challenge of maintaining consistency while achieving dense multi-view generation for improved human reconstruction, 
                we first introduce hybrid multi-view attention to facilitate both efficient and thorough information interchange across different views. 
                Additionally, we present a geometry-aware dual branch to perform concurrent generation in both RGB and normal domains, further enhancing consistency via geometry cues. 
                Last but not least, to address ill-shaped issues arising from inaccurate SMPL-X estimation that conflicts with the reference image, 
                we propose a novel iterative refinement strategy, which progressively optimizes SMPL-X accuracy and the quality and consistency of the generated multi-views. 
                Extensive experimental results demonstrate that our method significantly outperforms existing approaches in both novel view synthesis and subsequent human reconstruction tasks.
                <br>
            </p>
        </div>
    </div>

    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Method Overview</h2>
        <div class="grid-container-1">
            <img src="assets/pipeline.png">
            <p style="font-size: 15px;font-family: Ubuntu; text-align: justify;">
                Given a single human image, our proposed <strong>MagicMan</strong> utilizes a pre-trained 2D diffusion model with 3D human body prior to generate novel view images. 
                First, the reference image is fed into the denoising UNet via a reference UNet, with the viewpoint condition incorporated through camera embeddings. 
                The rendered normal and segmentation maps of the posed SMPL-X mesh that corresponds to the reference image are also provided as geometry guidance to facilitate 3D-awareness and consistency.
                To obtain dense and consistent novel view images, we modify the attention module to a more efficient hybrid 1D-3D attention (a) to establish comprehensive connections between multi-views, 
                and propose a geometry-aware dual branch (b) to also generate normal images in complementary to RGB images via geometry cues.
                Last but not least, a novel iterative refinement strategy (c) is proposed (only during inference) 
                to gradually update the initially estimated inaccurate SMPL-X pose and the synthesized novel view images, 
                substantially reducing the ill-shaped issues arising from unreliable SMPL-X estimates.
            </p>
        </div>
    </div>


    
    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Comparisons of Novel View Synthesis</h2>
        <div class="grid-container-1">
            <!-- wild -->
            <p class="myprompt nerf_text">Novel view results on in-the-wild data</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-2.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-3.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-4.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-5.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-wild-6.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>
            
            <!-- thuman -->
            <p class="myprompt nerf_text">Novel view results on THuman2.1</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-thuman-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-thuman-2.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-thuman-3.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>
        </div>

        <!-- custom -->
        <div class="grid-container-1">
            <p class="myprompt nerf_text">Novel view results on CustomHumans</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-nvs-customhuman-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>
        </div>

    </div>




    
    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Comparisons of 3D Human Reconstruction</h2>
        <div class="grid-container-1">
            <!-- wild -->
            <p class="myprompt nerf_text">Reconstruction results on in-the-wild data</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-2.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-3.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-4.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-5.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-6.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-wild-7.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <!-- THuman -->
            <p class="myprompt nerf_text">Reconstruction results on THuman2.1</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-2.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-3.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-4.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-5.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-6.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-7.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-thuman-8.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <!-- Custom -->
            <p class="myprompt nerf_text">Reconstruction results on CustomHumans</p>
            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-customhuman-1.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>

            <div>
                <video class="video" id="10" loop playsinline autoPlay muted controls
                src="assets/comp-rec-customhuman-2.mp4" onloadedmetadata="this.playbackRate=1" 
                onplay="resizeAndPlay(this)"></video> 
            </div>


        </div>
    </div>

    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf_abstract">BibTeX</h2>
        <div class="bibtex">
            <pre><code>
@misc{he2024magicman,
    title={MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement},
    author={Xu He and Xiaoyu Li and Di Kang and Jiangnan Ye and Chaopeng Zhang and Liyang Chen and Xiangjun Gao and Han Zhang and Zhiyong Wu and Haolin Zhuang},
    year={2024},
    eprint={2408.14211},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
            </code></pre>
        </div>
    </div>

</body>
<footer>
    This project page template is inspired by <a href="https://sweetdreamer3d.github.io/">SweetDreamer.</a>
</footer>

</html>
